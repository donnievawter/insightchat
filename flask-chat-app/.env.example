# InsightChat Configuration
# Copy this file to .env and adjust the values as needed

# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-here-change-this

# Ollama Configuration
OLLAMA_URL=http://localhost:11434/api/chat

# Default Model Configuration
# Set the default model to use when the chat starts
# This should match one of the available models from your Ollama instance
# Examples: llama3.2:latest, llama3.1:latest, mistral:latest, codellama:latest
DEFAULT_MODEL=llama3.2:latest

# RAG API Configuration (optional)
# Uncomment and set if you have a RAG service running
# The RAG service should accept POST requests to /query with {"prompt": "...", "k": 5}
# RAG_API_URL=http://localhost:8000

# Flask Debug Mode (set to False in production)
FLASK_DEBUG=True